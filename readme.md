### RNN

[Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks](https://openreview.net/pdf?id=B1l6qiR5F7)  （ICLR 2019）

### Transformer & Attention

[Multi-Granularity Self-Attention for Neural Machine Translation](https://arxiv.org/abs/1909.02222) (EMNLP 2019)

[Tree Transformer  Integrating Tree Structures into Self-Attention](https://arxiv.org/abs/1909.06639) (EMNLP  2019)

[Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://arxiv.org/abs/1905.09418) (ACL 2019)

[Self-Attention with Structural Position Representations](https://arxiv.org/abs/1909.00383) (EMNLP 2019)

[Are Sixteen Heads Really Better than One?](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1905.10650.pdf) [NIPS 2019]

[Towards Better Modeling Hierarchical Structure for Self-Attention](https://arxiv.org/abs/1909.01562) (EMNLP 2019)

[An Analysis of Encoder Representations in Transformer-Based Machine Translation](https://www.aclweb.org/anthology/W18-5431/) (EMNLP 2018)

[How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures？](https://link.zhihu.com/?target=https%3A//www.aclweb.org/anthology/P18-1167)（ACL 2018)

### Word Embedding

[Single Training Dimension Selection for Word Embedding with PCA](https://arxiv.org/abs/1909.01761) (EMNLP 2019)

### Neural Machine Translation

[Neural Machine Translation](https://arxiv.org/abs/1709.07809) （Tutorials by Philipp Koehn）

[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473) （ICLR 2015）

