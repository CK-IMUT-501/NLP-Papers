### Transformer & Attention

[Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://arxiv.org/abs/1905.09418) (ACL 2019)

[Self-Attention with Structural Position Representations](https://arxiv.org/abs/1909.00383) (EMNLP 2019)

[Are Sixteen Heads Really Better than One?](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1905.10650.pdf) [NIPS 2019]

[Towards Better Modeling Hierarchical Structure for Self-Attention](https://arxiv.org/abs/1909.01562) (EMNLP 2019)

[An Analysis of Encoder Representations in Transformer-Based Machine Translation](https://www.aclweb.org/anthology/W18-5431/) (EMNLP 2018)

[How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures？](https://link.zhihu.com/?target=https%3A//www.aclweb.org/anthology/P18-1167)（ACL 2018)

### Word Embedding

[Single Training Dimension Selection for Word Embedding with PCA](https://arxiv.org/abs/1909.01761) (EMNLP 2019)



